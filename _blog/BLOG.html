<h1>Performance comparison: Next.js alongside Fargate, Lambda and Lambda@Edge</h1>
<p><strong>Hosting Next.js isn&#39;t just about Vercel.</strong> True, they offer unmatched support for new Next.js features and a great developer experience…</p>
<p>…but:</p>
<ul>
<li>Did you know you can <strong>easily host your Next.js app on AWS essentially for free</strong> using AWS Lambda?</li>
<li>Do you know when it makes sense to use containers to host Next.js rather than hosting it serverlessly?</li>
</ul>
<p>Today, I will explore options on how Next.js can easily work alongside key AWS services: <strong>Lambda</strong>, <strong>Lambda@Edge</strong>, and <strong>Fargate</strong>. Using these services instead of going after managed solutions, such as Vercel, can give you more flexibility and save you a serious amount of money.</p>
<p>In this post, I&#39;ll deploy the same Next.js app using the mentioned AWS services. I&#39;ll run several performance tests on these setups and then figure out the costs for each environment. Ready to go?</p>
<h2>Deployment options</h2>
<p>For those of you who are not familiar with Next.js, here is a quick overview of deployment options when using Next.js.</p>
<p>As a developer deploying Next.js app, you can choose between:</p>
<ol>
<li>fully managed solutions for a hassle-free experience or</li>
<li>self-managed setups for complete control (can still be hassle-free, read on)</li>
</ol>
<h3>1. Fully managed solutions</h3>
<p><img src="resources/shock-vercel.jpg" alt="Monkey meme"></p>
<p>When it comes to the deployment of Next.js application, <a href="https://vercel.com/">Vercel</a> (comes from the creators of Next.js) stands out as the de facto standard, providing a seamless experience with its user-friendly interface, automatic scaling, and many other features. <a href="https://www.netlify.com/">Netlify</a> (and some other PaaS web hosting providers) also provides a similar solution.</p>
<p>While both Vercel and Netlify have generous free tiers and are great for many projects (I used them personally), there are a few things that could be a no-go for some (especially larger) projects:</p>
<ul>
<li>Both Vercel and Netlify can get pricey once you get out of the free tier.</li>
<li>In the free tier, Vercel does not allow for using ads on your site.</li>
<li>If you need private VPC for compliance, security, or other reasons, you will probably need to opt into an expensive enterprise subscription.</li>
</ul>
<p>There may also be other reasons why <strong>these managed solutions are not usable for your use case</strong>. That is why today, we will look at self-hosting in your own AWS account.</p>
<h3>2. Self-managed setup</h3>
<p>If you are unable/do not wish to use the fully managed solutions, you can always self-host your app on a cloud/service of your choice (in this post, we will focus on AWS). The main advantages of self-hosting are:</p>
<ul>
<li><strong>Flexibility</strong> - integrate easily with any other backend services offered by the cloud.</li>
<li><strong>Lower costs</strong> - much cheaper than Vercel or Netlify (especially at scale).</li>
</ul>
<p>The recommended self-hosting approach by the creators of Next.js is to <strong>containerize your app</strong> and deploy it anywhere you need - allowing you to deploy your app basically anywhere. It is an easy approach that can also be reasonably cheap.</p>
<p><strong>The problem:</strong> With Vercel or Netlify, your website is basically free when you have very low or no traffic at all. <strong>When using container, you have to pay even if it is running idly.</strong></p>
<p>Due to the problem above, there were multiple attempts to provide developers a way to <strong>self-host their Next.js app using AWS Lambda</strong>, giving you the benefits of Vercel...</p>
<ul>
<li>you only pay for what you use (pay per request)</li>
<li>unrestricted scaling(almost) tailored to your needs (thanks to the serverless nature of Lambda functions)</li>
</ul>
<p>... but for a lower price.</p>
<p><img src="resources/lambda-developer.jpg" alt="Developer Lambda hosting"></p>
<p>Unfortunately, <strong>packaging the Next.js app to make it runnable in AWS Lambda is not a trivial problem to solve</strong> due to the nature and internal functioning of the Next.js framework.</p>
<p>Luckily, tools such as <a href="https://stacktape.com/">Stacktape</a> (full disclosure, I am one of the developers) or <a href="https://sst.dev/">SST</a> are able to do so and provide developers a way to host the Next.js app using AWS Lambda a breeze.</p>
<p>They can do this thanks to <a href="https://open-next.js.org/">OpenNext</a> - an open-source adapter that enables developers to build the Next.js app in a way it is runnable in AWS Lambda function. Shout-out to its creators (also creators of SST) and its community.</p>
<hr>
<h2>Test environments</h2>
<p>In our test, we will be comparing 3 deployment setups of the Next.js app:</p>
<table>
<thead>
<tr>
<th>#</th>
<th>Environment type</th>
<th>Configuration</th>
<th>Example Project to Deploy</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>AWS Lambda</td>
<td>1024 MB memory, build using OpenNext</td>
<td><a href="https://github.com/stacktape/starter-nextjs-ssr-website-lambda">example project</a></td>
</tr>
<tr>
<td>2</td>
<td>AWS Lambda@Edge</td>
<td>1024 MB memory, build using OpenNext</td>
<td><a href="https://github.com/stacktape/starter-nextjs-ssr-website-lambda">example project</a></td>
</tr>
<tr>
<td>3</td>
<td>AWS ECS Fargate container</td>
<td>0.5 CPU, 1024 MB memory, HTTP API Gateway</td>
<td><a href="https://github.com/stacktape/starter-nextjs-ssr-website-container">example project</a></td>
</tr>
</tbody></table>
<p>All of the environments are set in the <code>eu-west-1</code> (Ireland) region, which means most of the infrastructure is localized in this region (understandably, the Lambda@Edge must be set up in the <code>us-east-1</code> region but is distributed across the globe).</p>
<p>All of the environments are production-ready and have CDN enabled. However, since I am measuring the performance of the underlying compute engines, <strong>I will send requests directly to the Lambda/Container origins to simulate how CloudFront(CDN) sends requests to these origins.</strong></p>
<p>For sending requests, I have set up two EC2 instances: one in each <code>eu-west-1</code> (Ireland) and <code>us-east-1</code> (Virginia) region.</p>
<hr>
<h2>Test 1 - Request/response time</h2>
<!-- In the first test, we will measure latency (response time) by sending requests to the Next.js app. -->

<p>The goal of the test is to compare an average response time from the origins when the load is low.</p>
<p>For regular <strong>Lambda</strong> and <strong>Container</strong> environments, I carry out the test twice:</p>
<ul>
<li><strong>from the region where environment is deployed</strong> (<code>eu-west-1</code>) - simulates a request from a user that is geographically close to the environment</li>
<li><strong>from region distant from environment</strong> (<code>us-east-1</code>) - simulates a request from a user that is geographically far away from the environment</li>
</ul>
<p>For the <strong>Lambda@Edge</strong> environment, I only carry out one test:</p>
<ul>
<li><strong>from region distant from environment</strong> (<code>us-east-1</code>) - simulates a request from a user that is geographically far away from the environment (but the Lambda function still runs at the Edge - close to the user)</li>
<li>I will not carry out the test from <code>eu-west-1</code>. Considering my testing methods, the test would be equal to the regular Lambda test.</li>
</ul>
<blockquote>
<p>Note that I am omitting the first response time due to Lambda functions <a href="https://docs.aws.amazon.com/lambda/latest/operatorguide/execution-environments.html#cold-start-latency">cold starts</a>. Cold starts can be lengthy, especially considering that the Next.js app Lambda zip package can be quite hefty (reducing the package size is one of the things that the OpenNext community is working on). <strong>The cold starts are something to consider, but for the purposes of this test, it would skew our results.</strong> In production environments, the problem with cold starts can be mitigated by using <a href="https://docs.stacktape.com/compute-resources/nextjs-website/#using-warmer">warmer</a>.</p>
<p>Requests are sent one at a time in a following way:</p>
<ul>
<li>For <code>Container</code> environment I send requests to <code>HTTP API Gateway</code> which delivers traffic to containers.</li>
<li>For <code>Lambda</code> environments I use <code>Lambda URL</code> (I realize that calling Lambda URL is not the way Cloudfront calls the Lambda@Edge, but it is the closest we can get ATM).</li>
</ul>
</blockquote>
<h3>Results</h3>
<table>
<thead>
<tr>
<th>Origin</th>
<th>Requests from</th>
<th>Avg. Latency (ms)</th>
</tr>
</thead>
<tbody><tr>
<td>Ireland Container</td>
<td>Ireland</td>
<td>57.94</td>
</tr>
<tr>
<td>Ireland Lambda</td>
<td>Ireland</td>
<td>86.27</td>
</tr>
<tr>
<td>Virginia Lambda (Lambda@Edge)</td>
<td>Virginia</td>
<td>317.45</td>
</tr>
<tr>
<td>Ireland Container</td>
<td>Virginia</td>
<td>377.83</td>
</tr>
<tr>
<td>Ireland Lambda</td>
<td>Virginia</td>
<td>432.91</td>
</tr>
</tbody></table>
<p>Our tests showed that responses to requests are quickest when they&#39;re made in the same region as the server (obviously). Containers are a little faster than Lambda, but not by much.</p>
<p><strong>So, why is Lambda@Edge slower, even though it processes requests in the request&#39;s origin region?</strong> It&#39;s because of how OpenNext and Lambda@Edge work behind the scenes.</p>
<p>First, let&#39;s look at how Next.js uses internal caching.</p>
<h3>Next.js caching</h3>
<p>To improve your application&#39;s performance and to reduce costs, Next.js employs multiple mechanisms for <strong>caching rendered work and data requests on the server</strong> (namely, <strong>fetch data cache</strong> and <strong>full route cache</strong>). As you will see, these caching mechanisms can slow down our Next.js app when using Lambda@Edge.</p>
<p><img src="resources/caching-overview.png" alt="Next.js caching mechanisms"></p>
<h3>Lambda cache problem</h3>
<p>Traditionally, the cache is persisted on the server that hosts the Next.js app. This is also the case for our <strong>Container</strong> environment.</p>
<p>However, AWS Lambda functions do not have persistent storage to store the cache. OpenNext developers came up with the following solution: use S3 bucket as a centralized storage for the cache data so that all Lambda invocations can access the data in a single place.</p>
<p>This solution has a small caveat when using Lambda@Edge. <strong>When the cache bucket is located in a different region from where the Lambda is executing, Lambda must retrieve the cache data across the region, resulting in increased latency.</strong> That being said, the test also shows that request/response is <strong>still faster using Lambda@Edge</strong> than sending an entire request and getting the response from the other region. Moreover, with routes where you do not use cache at all, using Lambda@Edge can be significantly faster.</p>
<p><img src="resources/next-edge-lambda-cache-bucket.png" alt="Edge lambda cache bucket"></p>
<hr>
<h2>Test 2 - Load testing</h2>
<p>To generate traffic for load testing, I will be using the NodeJS HTTP benchmarking tool <a href="https://www.npmjs.com/package/autocannon">autocannon</a> with the following options:</p>
<ul>
<li><strong>pipelining</strong> - to increase the load, we will set pipelining to 10. This means that each connection can send up to 10 requests without waiting for a response.</li>
<li><strong>connections</strong> - to control the requests per second, we will be adjusting(increasing) the number of connections (I will use the following number of connections: 3, 10, 30, 50, 100, 200, 300, 500)</li>
</ul>
<p>Tests will be carried out for <strong>Lambda</strong> and <strong>Container</strong> environments from our EC2 instance in the <code>eu-west-1</code> region:</p>
<ul>
<li>Each test takes 60 seconds.</li>
<li>Each request will target the same route path (root <code>/</code>). In other words, each request should target the <code>index</code> page of our Next.js app.</li>
<li>We will observe how the increasing load impacts both the <strong>latency</strong> and the <strong>error rate</strong> and search for thresholds of each environment.</li>
</ul>
<blockquote>
<p>Testing the <strong>Lambda@Edge</strong> environment does not make sense (the test would be same as for the <strong>Lambda</strong> environment).</p>
</blockquote>
<h3>Test 2 - Results</h3>
<p>When it comes to Lambda environments, the potential for scaling is huge, and there are basically no (hard) limits. What&#39;s more, <strong>every incoming request has the full memory and CPU power of Lambda function at its disposal</strong>. This means that even if you are performing more resource-intensive operations (DB operations, processing...), you can rely on resources being available.</p>
<p>On the other hand, when using a container, all of the requests come into the same container. Hence, the container resources are shared between the incoming requests.</p>
<blockquote>
<p>In real life scenario, you can scale your containers easily either vertically (giving more resources to the container) or more commonly horizontally (<strong>adding more containers and spreading the load between them</strong>). In this test, I will not be using any horizontal/vertical scaling for our container, as I want to see how much a single container can take.</p>
</blockquote>
<p><img src="resources/latency_plot.png" alt="Latencies - increased load"></p>
<h4>Lambda results</h4>
<p>The worse performance of Lambda in this test (Container environment has better latencies when the load is lower) is a question mark since the average duration of Lambda (observed in AWS Cloudwatch metrics) is <strong>around 25 ms</strong>. Therefore, having an overall <strong>latency of ~300ms</strong> when the load is low is somewhat surprising. The cold starts could be affecting the average, but even the <strong>minimal latency in the tests was over 200ms</strong>. This would suggest that the latency delay happens not during Lambda execution but somewhere else. Whether this is due to some Lambda URL limitations, standard network delay or something else, we were not able to find out.</p>
<p>When I increased the load to around 200 connections (~6000 req/sec), we can see that the Lambda latency starts to increase. I was not able to get through ~6500 req/sec. I thought this was due to Lambda Throttling, but I only got two 429 Error(Throttling Error code) out of more than 250,000 processed requests.</p>
<p>My next guess was that the duration of Lambda increased due to <strong>overloading the S3 cache bucket</strong>, which in turn caused increased latency. This guess was wrong, as the Lambda average duration only rose to about ~30ms during high load.</p>
<p>My conclusion right now is that <strong>this in fact is due to Lambda Throttling</strong>. It seems that Lambda URL (gateway mechanism behind it) simply <strong>waits until Lambda can be executed (instead of sending 429 Error)</strong> using some queuing mechanism - which would explain both: why we are not getting many 429 Errors and why there is a latency increase. However, I was unable to find any resources confirming this, only an <a href="https://repost.aws/questions/QUcL83FbaRTratTYprSLN-BA/lambda-function-url-throttling">unanswered question on AWS forum</a> asking basically about the same thing.</p>
<h4>Container results</h4>
<p>The previous graph shows that as the traffic ramps up, the average latency for the container starts to rise. We can see that already at around 70 connections (~700 req/sec), we are getting to the <strong>1-second latency</strong>. From there on, the latency rises quite quickly. In the next graph, you will see that the error rate (amount of 503 errors) due to overload goes up as well.</p>
<p><img src="resources/request_response_plot_sqrt.png" alt="Processed requests"></p>
<p>This graph also confirms that the container is able to process around 4500 requests per minute which corresponds to the ~700request/sec threshold, we estimated from previous graph.</p>
<blockquote>
<p>It should be noted that the my simulation conditions might not be completely the same as what users experience in their Next.js app. User&#39;s production app might be generally more complex - fetching data from APIs and databases, or performing more resource-intensive operations.</p>
</blockquote>
<hr>
<h2>Pricing Comparison</h2>
<p>In test 2, I estimated the number of requests per second a single container can handle. With this information, we can develop pricing formulas to decide when it&#39;s better to use Containers or Lambdas.</p>
<p>Let&#39;s start by creating formulas for calculating the price of environments.</p>
<p>We will be comparing 4 environments:</p>
<ol>
<li>Lambda environment (OpenNext)</li>
<li>Lambda@Edge environment (OpenNext)</li>
<li>Fargate container + HTTP API Gateway</li>
<li>Fargate container + Application Load Balancer</li>
</ol>
<blockquote>
<p>I have added the environment <code>Fargate container + Application Load Balancer</code> which was not previously shown in tests. I did it to provide a more complete picture for the reader (and HTTP API Gateway can easily be swapped with Application Load Balancer seamlessly).</p>
</blockquote>
<h3>1. Lambda environment price</h3>
<p>Official information from AWS:</p>
<ul>
<li>the cost of 1ms of Lambda execution (with memory 1024MB as used in the test) is ~$0.0000000167,</li>
<li>the cost of Lambda per million requests is $0,2,</li>
<li>the cost of the S3 bucket: $0,0004 per 1000 GET requests and $0,005 per 1000 PUT requests.</li>
</ul>
<p>Breakdown of formula for monthly costs (where x is number of requests per month):</p>
<table>
<thead>
<tr>
<th></th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>+</td>
<td><code>average_lambda_function_duration * 0.0000000167 * x</code></td>
<td>Price for execution time</td>
</tr>
<tr>
<td>+</td>
<td><code>0.0000002 * x</code></td>
<td>Price for each request</td>
</tr>
<tr>
<td>+</td>
<td><code>x * 0.0004 / 1000 + x / 50 * 0.005 / 1000</code></td>
<td>Price for cache bucket (Assuming 1 GET request per client request and 1 PUT request for every 50 client requests)</td>
</tr>
</tbody></table>
<h3>2. Lambda@Edge environment price</h3>
<p>Official information from AWS:</p>
<ul>
<li>the cost of 1ms of Lambda execution (with memory 1024MB as used in the test) is ~$0.00000005001,</li>
<li>the cost of Lambda per million requests - $0,6,</li>
<li>the cost of the S3 bucket: $0,0004 per 1000 GET requests and $0,005 per 1000 PUT requests.</li>
</ul>
<p>Breakdown of formula for monthly costs (where x is number of requests per month):</p>
<table>
<thead>
<tr>
<th></th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>+</td>
<td><code>average_lambda_function_duration * 0.00000005001 * x</code></td>
<td>Price for execution time</td>
</tr>
<tr>
<td>+</td>
<td><code>0.0000006 * x</code></td>
<td>Price for each request</td>
</tr>
<tr>
<td>+</td>
<td><code>x * 0.0004 / 1000 + x / 50 * 0.005 / 1000</code></td>
<td>Price for cache bucket (Assuming 1 GET request per client request and 1 PUT request for every 50 client requests)</td>
</tr>
</tbody></table>
<h3>3. Fargate container + HTTP API Gateway price</h3>
<p>Official information from AWS:</p>
<ul>
<li>the cost of the Fargate container we used in our test is ~$0,0244/hour or ~$17,5/month,</li>
<li>the cost of HTTP API Gateway is $1 for 1 000 000 requests.</li>
</ul>
<p>Breakdown of formula for monthly costs (where x is number of requests per month):</p>
<table>
<thead>
<tr>
<th></th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>+</td>
<td><code>17.5 * ceil((x+1) / (2592000 * number_of_req_per_sec_handled_by_single_container))</code></td>
<td>Fees for Fargate container (2592000 is the number of seconds in the month)</td>
</tr>
<tr>
<td>+</td>
<td><code>x / 1000000</code></td>
<td>Price for HTTP API Gateway</td>
</tr>
</tbody></table>
<h3>4. Fargate container + Application Load Balancer price</h3>
<p>Official information from AWS:</p>
<ul>
<li>the cost of the Fargate container we used in our test is ~$0,0244/hour or ~$17,5/month,</li>
<li>the cost of Application Load Balancer is $16,2 + 0,008LCU/hour(or 5,76/month).</li>
</ul>
<p>Breakdown of formula for monthly costs (where x is number of requests per month):</p>
<table>
<thead>
<tr>
<th></th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>+</td>
<td><code>17.5 * ceil((x+1) / (2592000 * number_of_req_per_sec_handled_by_single_container))</code></td>
<td>Fees for Fargate container (2592000 is the number of seconds in the month)</td>
</tr>
<tr>
<td>+</td>
<td><code>16.2</code></td>
<td>Flat monthly fee for Application Load Balancer</td>
</tr>
<tr>
<td>+</td>
<td><code>x / (2592000 * 1000) * number_of_lcus_needed_for_thousand_req_sec * 5.76</code></td>
<td>Fee for LCUs</td>
</tr>
</tbody></table>
<h3>Pricing Comparison Results</h3>
<p>I introduced new constants in the formulas. For formulas to be complete, we have to set these constants to specific values. I will also adjust them later on to show how they influence the final results :</p>
<ul>
<li><p><code>average_lambda_function_duration</code> - In the test, the average lambda duration was around 27ms. However, the average lambda duration might be higher in real scenarios since you will generate more complex websites and fetch data from APIs and databases. We will set it to <strong>50ms</strong> for this simulation.</p>
</li>
<li><p><code>number_of_lcus_needed_for_thousand_req_sec</code> - we will set this to <strong>50</strong>. This can be more or less according to the amount of data you will be transferring on average in each request.</p>
</li>
<li><p><code>number_of_req_per_sec_handled_by_single_container</code> - In the test, the container was able to handle more than 500 req/sec. However, as with the <code>average_lambda_function_duration</code>, these values might differ in production due to more complex operations. We will set this to <strong>100</strong>.</p>
</li>
</ul>
<p><img src="resources/pricing-comparison-50-50-100.png" alt="Pricing comparison"></p>
<p>We can see that if the average load is low, the Lambda environments are the cheapest. This is the case for most websites, where <strong>most of the content can be cached on CDN</strong>.</p>
<p>When your Lambda function gets more traffic, Lambda environment can get pricier. The amount of traffic the function receives depends on your Next.js app. Usually, more traffic happens because:</p>
<ul>
<li>You&#39;re not using CDN cache,</li>
<li>You have quick revalidation times,</li>
<li>Your app uses a lot of API calls that can&#39;t be cached.</li>
</ul>
<p><strong>Already at 3 requests/second(Lambda@Edge) and 9 requests/second(Lambda), using containers with a load balancer seems to be a more suitable option.</strong></p>
<blockquote>
<p>For many production websites, there might not be many requests coming to the origin since most of content is cached on CDN, so using Lambda might be cheapest.</p>
</blockquote>
<p>It is important to note that the graph can look very different if you change the constants that we have set.</p>
<p>For example, changing <code>average_lambda_function_duration,</code> <code>number_of_req_per_sec_handled_by_single_container</code>, or increasing/decreasing requests sent to the cache bucket - <strong>all these constants depend on your specific setup and influence the final price</strong>.</p>
<p>The following graph uses the same formulas, but <code>average_lambda_function_duration</code> is set to <strong>30ms</strong>, and <code>number_of_req_per_sec_handled_by_single_container</code> is set to <strong>50</strong>.</p>
<p><img src="resources/pricing-comparison-30-50-50.png" alt="Pricing comparison - adjusted constants"></p>
<p>You should also note that our price estimations do not include Data Transfer fees that you will receive from AWS, as these should be similar for all environments and are not a differentiator.</p>
<hr>
<h2>Conclusion</h2>
<p>Our tests shed light on the performance of different Next.js hosting options on AWS. Remember, your Next.js app&#39;s results might differ due to various factors.</p>
<p>Key takeaways from our tests:</p>
<p><strong>Lambda environments work best when:</strong></p>
<ul>
<li>Your Lambda doesn&#39;t consistently get 5-10 requests per second (true for most websites, as your content will be cached on CDN).</li>
<li>You need quick scaling for unpredictable traffic loads (Lambda scales instantly, unlike containers).</li>
<li>Be cautious with Lambda@Edge as costs can rise quickly. It&#39;s best for global apps where users benefit from edge locations.</li>
</ul>
<p><strong>Container environments are a good choice if:</strong></p>
<ul>
<li>Your traffic consistently exceeds 5-10 requests per second.</li>
<li>You want to avoid Lambda cold starts.</li>
<li>Your traffic is steady without sudden spikes, as containers scale slower than Lambda.</li>
</ul>
<p>Thanks for making it this far! If you spot any errors in our tests or want to see more comparisons, let us know.</p>
